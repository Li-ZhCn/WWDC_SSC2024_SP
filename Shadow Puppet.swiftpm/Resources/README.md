#  README

## Features and technologies used.
* Vision: To detect and recognize user's fingers, allowing user to control the puppet by pinching and moving hands.
* SwiftUI: Most of the views were made based on SwiftUI.
* SpriteKit: Puppet scene was made by SpriteKit. Inverse kinematics were used to manipulate the puppet.
* UIKit: Live camera preview was used as input of Vision Framework and also to help user to play.
* Accelerate: The output of Vision Framework was filtered by certain method, in which Accelerate played an essential part.

## Open source software used
* ToyViewer: This is used to precisely rotate and resize images.
* GarageBand: All the background music and sound effects are made it.

## Others
Some images of puppet were made based on pictures shot by myself in the Museum of Wang Pi Yeng in Sichuan, China.


